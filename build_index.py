import os
import pandas as pd
import re
import faiss
import pickle
from docx import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from langchain_core.documents import Document as LangchainDocument

# Khá»Ÿi táº¡o model embedding
embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a file
data_dir = r"C:\Users\Admin\Documents\Python\LLM\BÃ¡o cÃ¡o tÃ i chÃ­nh"

# TÃªn file index vÃ  chunks
index_path = "vector.index"
chunks_path = "chunks.pkl"
processed_files_path = "processed_files.pkl"

# Táº£i FAISS index vÃ  cÃ¡c chunks cÅ© náº¿u cÃ³
if os.path.exists(index_path) and os.path.exists(chunks_path):
    print("âœ… ÄÃ£ cÃ³ FAISS index. Äang táº£i...")
    index = faiss.read_index(index_path)
    with open(chunks_path, "rb") as f:
        chunks = pickle.load(f)
    if os.path.exists(processed_files_path):
        with open(processed_files_path, "rb") as f:
            processed_files = pickle.load(f)
    else:
        processed_files = set()
else:
    print("ğŸ†• ChÆ°a cÃ³ FAISS index. Táº¡o má»›i.")
    index = None
    chunks = []
    processed_files = set()

# HÃ m Ä‘á»c file docx
def read_docx(file_path):
    doc = Document(file_path)
    return "\n".join(para.text for para in doc.paragraphs)

# Tá»•ng há»£p cÃ¡c vÄƒn báº£n má»›i
new_paragraphs = []
new_files = []

for file_name in os.listdir(data_dir):
    if file_name in processed_files:
        continue  # Bá» qua file Ä‘Ã£ xá»­ lÃ½

    file_path = os.path.join(data_dir, file_name)
    print(f"ğŸ“„ Äang xá»­ lÃ½: {file_name}")
    try:
        if file_name.endswith(".docx"):
            text = read_docx(file_path)

        elif file_name.endswith(".csv"):
            df = pd.read_csv(file_path)
            text = "\n".join(
                " | ".join([f"{col}: {row[col]}" for col in df.columns if pd.notnull(row[col])])
                for _, row in df.iterrows()
            )

        elif file_name.endswith((".xlsx", ".xls")):
            df = pd.read_excel(file_path)
            text = "\n".join(
                " | ".join([f"{col}: {row[col]}" for col in df.columns if pd.notnull(row[col])])
                for _, row in df.iterrows()
            )

        elif file_name.endswith(".txt"):
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()

        else:
            print(f"âš ï¸ Bá» qua file khÃ´ng há»— trá»£: {file_name}")
            continue

        # LÃ m sáº¡ch vÄƒn báº£n
        for line in text.split("\n"):
            cleaned = re.sub(r'[E\[\]Là­®]', '', line)
            cleaned = re.sub(r'\s+', ' ', cleaned.strip())
            if cleaned:
                new_paragraphs.append(cleaned)

        new_files.append(file_name)

    except Exception as e:
        print(f"âŒ Lá»—i xá»­ lÃ½ file {file_name}: {e}")

# Náº¿u cÃ³ dá»¯ liá»‡u má»›i thÃ¬ nhÃºng vÃ  thÃªm vÃ o FAISS index
if new_paragraphs:
    documents = [LangchainDocument(page_content=p) for p in new_paragraphs]
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    new_chunks = splitter.split_documents(documents)
    texts = [chunk.page_content for chunk in new_chunks]
    print(f"ğŸ§  Äang táº¡o embedding cho {len(texts)} Ä‘oáº¡n...")

    embeddings = embed_model.encode(texts, show_progress_bar=True)

    if index is None:
        index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)

    # Cáº­p nháº­t vÃ  lÆ°u láº¡i
    chunks.extend(new_chunks)
    processed_files.update(new_files)

    faiss.write_index(index, index_path)
    with open(chunks_path, "wb") as f:
        pickle.dump(chunks, f)
    with open(processed_files_path, "wb") as f:
        pickle.dump(processed_files, f)

    print(f"âœ… ÄÃ£ thÃªm {len(new_chunks)} Ä‘oáº¡n tá»« {len(new_files)} file má»›i.")
else:
    print("ğŸš« KhÃ´ng cÃ³ file má»›i nÃ o cáº§n xá»­ lÃ½.")